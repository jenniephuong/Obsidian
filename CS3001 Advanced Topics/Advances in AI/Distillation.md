> Training smaller, more efficient (student) models to mimic the behaviour and reasoning patters of the larger models by using it as a teacher - transferring the knowledge and capabilities parameter model into more compact architectures

![[Pasted image 20250421143120.png]]

how does distillation work
- given some unlabelled data, the teacher LLM is asked to label it. this synthetically labelled data is given to train the student model

pros: 
- generates predictions much faster, good for real-time applications or resource-contrained devices 
- reduced parameters = requires fewer computational and environmental resources (energy consumption), easier to store and mange
- less expensive to host an LLM with fewer parameters via API

cons: 
- predictions are usually not as good as the original LLM's 
- the student is limited to the teacher e.g. generalised LLMS don't do well with specialised tasks 
- still requires lots of unlabelled data
- the OG LLM labelled data might be sensitive and not usable 

like a subset of knowledge?

Key Roles in LLMs
1. enhancing capabilities
2. traditional compression for efficiency
3. emerging trend of self-improvement via self-generate knowledge

what can be done to mitigate knowledge loss 
- augment the data generated by the teacher model to provide broader range of training examples 
- iterative distillation - can capture more of the teacher's knowledge
- adjust hyperparameters like temperature and learning rate 
	- temperate - smoothness of probability distribution and creativity of responses 
	- learning rate - balance speed and stability of training process so the model can converge without under or over fitting