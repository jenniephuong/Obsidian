https://www.researchgate.net/publication/226920353_Evaluating_Augmented_Reality_Systems

distinction between different types of evaluation measures 
- objective - reliable and repeatable quantititative observations 
- subjective - including questionnaires to understand human judgement 
- qualitative analysis - structured observations like discussions or interviews 
- expert based usability evaluation - cognitive walkthrough

How can MR systems be evaluated?
- behavioural metrics - gaze direction
- psychological metrics - ECG, GSR for galvanic skin response to measure arousal , Electroencephalography (EEG) to measure brain activity 
- performance metrics - measuring task complextion time, success rates, accuracy
-  level of collaboration between participants - by measuring metrics during workshops e.g. capture video of the workshop and transcribe the discussions to measure number of gestures and utterances - later qualitative analysis 
- use of surveys and questionnaires to determine enjoyment, realism, immersion, sense of _presence_
- UX review - including expert reivew
- human perception and cognition during low level MR tasks
- system functionality and design evaluation
- functional task alignment - does the MR allow the task to be conducted in how it would originally be conduced in reality 

commonly done by inviting participants to participate in a workshop or practical with a planned MR scenario for them to go through, and then measure their performance and other metrics as they go, along with conducting post-experience interviewers and questionnaires 

has cons of practicality (EEG sensors tiring for participants to wear along with the HMD), and universality, measuring presence can only be used in high stress scenarios, post MR surveys not inclusive of fluctuations occurring during the MR experience  

It's difficult to apply traditional usability evaluation methods (heuristics) to AR/MR because MR involves non-traditional interaction models WIMP (windows icon menu pointing device) e.g. hand gestures that manipulate objects in a 3D space > window point and click
- also no methods to evaluate collaboration

Unlike WIMP interfaces that share the same common attributes, AR interfaces can vary drastically (in both hardware and software)

the most suitable evaluation technique always depends on context 
- but measuring **user task performance** is the most popular form of AR evaluation 
- perhaps impossible to outline a single evaluation framework applicable to all AR systems 

Solution - create a general evaluation framework that can guide those evaluating MR systems

Stanney et al. have created a computerized system to assist in the evaluation of virtual environments. MAUVE (Multi-criteria Assessment of Usability for Virtual Environments) ![[Pasted image 20250414160630.png]]


![[Pasted image 20250414162011.png]]

#flash 